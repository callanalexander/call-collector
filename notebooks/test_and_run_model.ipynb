{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e58a5c-fa2d-454f-a5ff-11a163fe8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on test dataset 1 \n",
    "#you can find and download this testing dataset in the test folder\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model_path, test_data_path, save_path):\n",
    "    # Load the saved model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Setup test data generator\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(test_generator)\n",
    "    predicted_classes = (predictions > 0.5).astype(int)\n",
    "    true_classes = test_generator.classes\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_classes, predicted_classes))\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Calculate metrics from confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate accuracy correctly\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(f\"\\nDetailed Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(true_classes, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Save the plot instead of displaying it\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nAUC-ROC Score: {roc_auc:.4f}\")\n",
    "    print(f\"ROC curve plot saved to: {save_path}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'auc_roc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "\n",
    "# Paths to your model and test data\n",
    "model_path = r'path_to_your_model'\n",
    "test_data_path = r'path_to_test_dataset_1'\n",
    "save_path = r'path_to_save_location'\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_model(model_path, test_data_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089724e-a649-4c51-8335-a8ceda466863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infer on hour long audio files\n",
    "#this code will run the model recursively over a folder of .wav files and generate a prediction csv for each file\n",
    "#you can test our models in the 'models' folder, and run them over the test2 dataset in the 'test' folder'\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image, ImageEnhance\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import gc\n",
    "import logging\n",
    "import traceback\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Global variables\n",
    "model = None\n",
    "target_size = (224, 224)\n",
    "brightness_factor = 0.8\n",
    "contrast_factor = 2.0\n",
    "batch_size = 32\n",
    "duration = 5\n",
    "overlap = 2.5\n",
    "sample_rate = 48000\n",
    "\n",
    "# Thread-local storage for model\n",
    "thread_local = threading.local()\n",
    "\n",
    "def get_model():\n",
    "    if not hasattr(thread_local, 'model'):\n",
    "        thread_local.model = load_model(r'path_to_your_model.keras')\n",
    "    return thread_local.model\n",
    "\n",
    "def create_and_process_spectrogram(y, sr):\n",
    "    try:\n",
    "        if len(y) == 0:\n",
    "            logging.warning(\"Audio segment is empty\")\n",
    "            return None\n",
    "\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, n_mels=256, fmax=24000, hop_length=512)\n",
    "        S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(3.5, 3.5))\n",
    "        img = ax.imshow(S_DB, aspect='auto', origin='lower', cmap='viridis')\n",
    "        ax.axis('off')\n",
    "        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "\n",
    "        img = Image.fromarray(img_array).convert('RGB')\n",
    "        enhancer = ImageEnhance.Brightness(img)\n",
    "        img = enhancer.enhance(brightness_factor)\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        img = enhancer.enhance(contrast_factor)\n",
    "        img = img.resize(target_size)\n",
    "\n",
    "        return img_to_array(img)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in create_and_process_spectrogram: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def predict_chunk(chunk, sr, start_time):\n",
    "    try:\n",
    "        images = []\n",
    "        segment_times = []\n",
    "        segment_duration = 5\n",
    "        segment_samples = int(sr * segment_duration)\n",
    "        \n",
    "        for i in range(0, len(chunk), int(sr * (segment_duration - overlap))):\n",
    "            segment = chunk[i:i + segment_samples]\n",
    "            if len(segment) < segment_samples:\n",
    "                break\n",
    "            segment_time = start_time + i / sr\n",
    "            segment_times.append(segment_time)\n",
    "            img_array = create_and_process_spectrogram(segment, sr)\n",
    "            if img_array is not None:\n",
    "                images.append(img_array)\n",
    "\n",
    "        if not images:\n",
    "            return []\n",
    "\n",
    "        images = np.array(images)\n",
    "        images = (images / 255.0) * 2.0 - 1.0\n",
    "        model = get_model()\n",
    "        predictions = model.predict(images, batch_size=batch_size)\n",
    "        results = [(time, pred[0]) for time, pred in zip(segment_times, predictions)]\n",
    "        \n",
    "        del images\n",
    "        gc.collect()\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in predict_chunk at time {start_time}: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return []\n",
    "\n",
    "def process_file(filename, test_dir):\n",
    "    file_path = os.path.join(test_dir, filename)\n",
    "    try:\n",
    "        logging.info(f\"Started processing {filename}\")\n",
    "        chunk_duration = 300  # 5 minutes\n",
    "        \n",
    "        if not os.path.isfile(file_path) or not os.access(file_path, os.R_OK):\n",
    "            logging.error(f\"File does not exist or is not readable: {file_path}\")\n",
    "            return\n",
    "\n",
    "        all_results = []\n",
    "        start_time = 0\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                y, sr = librosa.load(file_path, sr=sample_rate, offset=start_time, duration=chunk_duration)\n",
    "                if len(y) == 0:\n",
    "                    break\n",
    "                chunk_results = predict_chunk(y, sr, start_time)\n",
    "                all_results.extend([(time, pred) for time, pred in chunk_results])\n",
    "                logging.info(f\"Processed chunk at {start_time} of {filename}, got {len(chunk_results)} results\")\n",
    "                start_time += chunk_duration\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing chunk at {start_time} of {filename}: {str(e)}\")\n",
    "                break\n",
    "\n",
    "        if all_results:\n",
    "            positive_count = sum(1 for _, p in all_results if p > 0.01)\n",
    "            logging.info(f\"Detected {positive_count} potential owl calls out of {len(all_results)} segments in {filename}\")\n",
    "        else:\n",
    "            logging.warning(f\"No results produced for {filename}\")\n",
    "\n",
    "        # Write results to a CSV file for this audio file\n",
    "        csv_filename = f\"{os.path.splitext(filename)[0]}_predictions.csv\"\n",
    "        with open(csv_filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Start Time (s)', 'Prediction'])\n",
    "            for time, prediction in all_results:\n",
    "                writer.writerow([time, prediction])\n",
    "        logging.info(f\"Wrote {len(all_results)} results to {csv_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {filename}: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    test_dir = r'C:\\Users\\calla\\Dropbox\\2024\\powerfulowl\\2025_post_clusterv1\\audio'\n",
    "    logging.info(f\"Looking for WAV files in: {test_dir}\")\n",
    "\n",
    "    if not os.path.exists(test_dir):\n",
    "        logging.error(f\"Directory does not exist: {test_dir}\")\n",
    "        return\n",
    "\n",
    "    wav_files = [f for f in os.listdir(test_dir) if f.endswith('.wav')]\n",
    "    \n",
    "    if not wav_files:\n",
    "        logging.warning(f\"No WAV files found in {test_dir}\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Found {len(wav_files)} WAV files: {', '.join(wav_files)}\")\n",
    "\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "            futures = [executor.submit(process_file, filename, test_dir) for filename in wav_files]\n",
    "            concurrent.futures.wait(futures)\n",
    "        \n",
    "        logging.info(\"All files have been processed.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main function: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d5529-ae65-419f-a047-f243b7fd6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate snippets - fix to make recursive\n",
    "\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# Define the paths\n",
    "folder_path = r'C:\\Users\\calla\\Dropbox\\2024\\powerfulowl\\2025_post_cluster_v5\\5.1'\n",
    "output_folder = os.path.join(folder_path, 'snippets/')\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to extract snippets\n",
    "def extract_snippets(csv_file, threshold=0.5, duration=5000, margin=500):\n",
    "    base_name = os.path.splitext(os.path.basename(csv_file))[0].replace('_predictions', '')\n",
    "    wav_file = os.path.join(folder_path, base_name + '.WAV')\n",
    "    \n",
    "    if not os.path.exists(wav_file):\n",
    "        print(f\"WAV file for {csv_file} not found.\")\n",
    "        return\n",
    "    \n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Load the WAV file\n",
    "    audio = AudioSegment.from_wav(wav_file)\n",
    "    \n",
    "    # Find segments above threshold\n",
    "    df['above_threshold'] = df['Prediction'] > threshold\n",
    "    \n",
    "    # Group consecutive segments\n",
    "    df['group'] = (df['above_threshold'] != df['above_threshold'].shift()).cumsum()\n",
    "    \n",
    "    # Process each group of segments\n",
    "    for group, group_df in df[df['above_threshold']].groupby('group'):\n",
    "        start_time = group_df['Start Time (s)'].min()\n",
    "        end_time = group_df['Start Time (s)'].max() + 5  # Add 5 seconds for the last segment\n",
    "        max_prediction = group_df['Prediction'].max()\n",
    "        \n",
    "        # Calculate start and end times in milliseconds\n",
    "        start_ms = max(0, (start_time - margin/1000) * 1000)\n",
    "        end_ms = min((end_time + margin/1000) * 1000, len(audio))\n",
    "        \n",
    "        # Extract snippet\n",
    "        snippet = audio[start_ms:end_ms]\n",
    "        \n",
    "        # Create output filename\n",
    "        output_filename = f\"{base_name}_{start_time:.2f}-{end_time:.2f}_score{max_prediction:.4f}.wav\"\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        \n",
    "        # Export snippet\n",
    "        snippet.export(output_path, format=\"wav\")\n",
    "        print(f\"Exported: {output_filename}\")\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*_predictions.csv'))\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Processing: {csv_file}\")\n",
    "    extract_snippets(csv_file)\n",
    "\n",
    "print(\"Snippets extraction for all files completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
