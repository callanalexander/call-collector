{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc472b2-ffed-4b2f-aa3b-ee152ecd58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##generate ROIS from .wav Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3b4f7-ca80-4ec4-872e-c83a49a5517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate snippets from predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2314ce-090b-4333-b8cb-3b69ad613ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROI generation \n",
    "\n",
    "#run this block to generate a CSV of ROIS (regions of interest) for your .wav file snippets\n",
    "#an example test dataset of snippets can be downloaded in the supplementary materials\n",
    "#if running this on your own data, we suggest placing your snippets in the following structure: /audio/sites/yoursitename/snippets\n",
    "#to populate all rows of the csv you will need your .wav files named in the following convention prefix_filename_score0.0000 where score is the confidence score assigned by your classifier. \n",
    "#eg. POWL_20200423_175404_385.00-390.00_score0.4516.wav\n",
    "#if you want to skip this step you can use our example ROI .csv file in the following step\n",
    "\n",
    "# Set input and output paths here\n",
    "INPUT_PATH = r\"path_to_your_snippets\"  # Path to folder containing .wav files - you can use our example_snippets folder \n",
    "OUTPUT_PATH = r\"path_to_your_snippets\"   # Path to save output csv file - you will need this csv file in the next step \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from maad import sound, features as maad_features, rois\n",
    "from maad.util import power2dB, plot2d, format_features, overlay_rois, rand_cmap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "def process_file(file_path, bg_smooth_coef=0.1, smooth_std=0.8, bin_std=1.5, bin_per=0.2, min_roi=30):\n",
    "    try:\n",
    "        s, fs = sound.load(file_path)\n",
    "        s_filt = sound.select_bandwidth(s, fs, fcut=1000, forder=3, ftype='lowpass')\n",
    "\n",
    "        db_max = 70\n",
    "        Sxx, tn, fn, ext = sound.spectrogram(s_filt, fs, nperseg=2048, noverlap=1024)\n",
    "        Sxx_db = power2dB(Sxx, db_range=db_max) + db_max\n",
    "\n",
    "        Sxx_db_rmbg, _, _ = sound.remove_background(Sxx_db, smooth_coef=bg_smooth_coef)\n",
    "        Sxx_db_smooth = sound.smooth(Sxx_db_rmbg, std=smooth_std)\n",
    "        im_mask = rois.create_mask(im=Sxx_db_smooth, mode_bin='relative', bin_std=bin_std, bin_per=bin_per)\n",
    "        im_rois, df_rois = rois.select_rois(im_mask, min_roi=min_roi, max_roi=None)\n",
    "\n",
    "        if df_rois.empty:\n",
    "            print(f\"No ROIs found in file {file_path}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        df_rois = format_features(df_rois, tn, fn)\n",
    "\n",
    "        df_shape, params = maad_features.shape_features(Sxx_db, resolution='low', rois=df_rois)\n",
    "        df_centroid = maad_features.centroid_features(Sxx_db, df_rois)\n",
    "\n",
    "        if df_shape.empty or df_centroid.empty:\n",
    "            print(f\"Empty shape or centroid features for file {file_path}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        median_freq = fn[np.round(df_centroid.centroid_y).astype(int)]\n",
    "        df_centroid['centroid_freq'] = median_freq/fn[-1]\n",
    "\n",
    "        features_df = df_shape.join(df_centroid, lsuffix='_shape', rsuffix='_centroid')\n",
    "        \n",
    "        return features_df, Sxx_db, ext, df_rois\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None\n",
    "\n",
    "def main():\n",
    "    folder_path = INPUT_PATH\n",
    "    output_path = OUTPUT_PATH\n",
    "    \n",
    "    print(f\"Input path: {folder_path}\")\n",
    "    print(f\"Output path: {output_path}\")\n",
    "    \n",
    "    all_features = []\n",
    "    file_data = []  # To store data for spectrogram generation\n",
    "\n",
    "    wav_files = [f for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "    print(f\"Found {len(wav_files)} .wav files in the folder.\")\n",
    "\n",
    "    for filename in wav_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        \n",
    "        features, Sxx_db, ext, df_rois = process_file(\n",
    "            file_path, \n",
    "            bg_smooth_coef=0.4, \n",
    "            smooth_std=0.8, \n",
    "            bin_std=1.5, \n",
    "            bin_per=0.2, \n",
    "            min_roi=20\n",
    "        )\n",
    "        \n",
    "        if features is not None:\n",
    "            print(f\"Successfully processed {filename}. Shape: {features.shape}\")\n",
    "            features['file_name'] = filename\n",
    "            all_features.append(features)\n",
    "            file_data.append((filename, Sxx_db, ext, df_rois))\n",
    "        else:\n",
    "            print(f\"Skipping file {filename} due to processing error.\")\n",
    "        \n",
    "        print(\"---\")\n",
    "\n",
    "    print(f\"Successfully processed {len(all_features)} out of {len(wav_files)} files.\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No features were extracted. Please check your input files and processing function.\")\n",
    "    else:\n",
    "        combined_features = pd.concat(all_features, ignore_index=True)\n",
    "        features_csv_path = os.path.join(output_path, 'all_features.csv')\n",
    "        combined_features.to_csv(features_csv_path, index=False)\n",
    "        print(f\"Saved features to {features_csv_path}. Shape: {combined_features.shape}\")\n",
    "\n",
    "        # Prepare data for t-SNE and clustering\n",
    "        X = combined_features.loc[:, combined_features.columns.str.startswith('shp')]\n",
    "        X = X.join(combined_features.centroid_freq)\n",
    "\n",
    "        # Perform t-SNE\n",
    "        tsne = TSNE(n_components=2, perplexity=12, init='pca', verbose=True)\n",
    "        Y = tsne.fit_transform(X)\n",
    "\n",
    "        # Perform DBSCAN clustering\n",
    "        cluster = DBSCAN(eps=5, min_samples=4).fit(Y)\n",
    "        print('Number of soundtypes found:', np.unique(cluster.labels_).size)\n",
    "\n",
    "        # Add cluster labels to the combined features\n",
    "        combined_features['cluster'] = cluster.labels_\n",
    "\n",
    "        # Save the final result with cluster labels\n",
    "        clustered_csv_path = os.path.join(output_path, 'all_features_with_clusters.csv')\n",
    "        combined_features.to_csv(clustered_csv_path, index=False)\n",
    "        print(f\"Saved clustered features to {clustered_csv_path}\")\n",
    "\n",
    "        # Visualize clustering results\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(Y[:,0], Y[:,1], c=cluster.labels_, cmap=rand_cmap(np.unique(cluster.labels_).size, first_color_black=False), alpha=0.8)\n",
    "        plt.xlabel('t-SNE dimension 1')\n",
    "        plt.ylabel('t-SNE dimension 2')\n",
    "        plt.title('Clustering Results for All Files')\n",
    "        plt.colorbar(scatter)\n",
    "        clustering_plot_path = os.path.join(output_path, 'clustering_results.png')\n",
    "        plt.savefig(clustering_plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved clustering visualization to {clustering_plot_path}\")\n",
    "\n",
    "        # Generate spectrograms with labeled ROIs for the first few files\n",
    "        num_spectrograms = 10\n",
    "        for i, (filename, Sxx_db, ext, df_rois) in enumerate(file_data[:num_spectrograms]):\n",
    "            # Get cluster labels for this file\n",
    "            file_features = combined_features[combined_features['file_name'] == filename]\n",
    "            df_rois['label'] = file_features['cluster'].astype(str).values\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            plot2d(Sxx_db, ax=ax, extent=ext, vmin=0, vmax=70)\n",
    "            overlay_rois(Sxx_db, ax=ax, rois=df_rois, ext=ext)\n",
    "            plt.title(f\"Spectrogram with Clustered ROIs: {filename}\")\n",
    "            plt.tight_layout()\n",
    "            spectrogram_path = os.path.join(output_path, f\"{filename}_clustered_spectrogram.png\")\n",
    "            plt.savefig(spectrogram_path)\n",
    "            plt.close()\n",
    "            print(f\"Saved clustered spectrogram to {spectrogram_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc26cd-9048-4c8e-a8db-31174acecbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster gui\n",
    "\n",
    "#run the code block below to open a simple GUI that will cluster your ROIs and allow for inspection \n",
    "#if you want to colour the clusters by other variables, you can wrangle data into your csv file as needed - eg site_name or ground_truth\n",
    "#we have included a sample csv (verified_rois) in the supplementary materials which you can use to test this feature\n",
    "#make sure to full screen or some buttons may be missing, this will be fixed hopefully in future updates\n",
    "#if you don't want to use the gui, a code just to generate the plots is available in the following cell \n",
    "#you do not need to change any paths in the code, just run the gui and then use the dropdowns to select your csv in the 'data' section, and the folder where the audio snippets were in the 'audio section' \n",
    "#you can also tweak the UMAP and HDBSCAN parameters. Example parameters are available in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabccc8-4350-4efe-b3b5-866c2c8e1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive Agg backend\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "import hdbscan\n",
    "from maad import sound\n",
    "from maad.util import power2dB, plot2d\n",
    "import threading\n",
    "import queue\n",
    "import soundfile as sf\n",
    "import sounddevice as sd\n",
    "\n",
    "class ClusterAnalysisGUI:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        self.master.title(\"Owl Call Cluster Analysis Tool\")\n",
    "        self.master.geometry(\"1400x900\")\n",
    "        self.master.config(bg=\"#f0f0f0\")\n",
    "        \n",
    "        # Data storage\n",
    "        self.df = None\n",
    "        self.base_dir = None\n",
    "        self.clusterer = None\n",
    "        self.reducer = None\n",
    "        self.X_scaled = None\n",
    "        self.embedding = None\n",
    "        self.clusters = None\n",
    "        self.progress_queue = queue.Queue()\n",
    "        self.column_categories = {'numeric': [], 'categorical': []}\n",
    "        \n",
    "        # Feature groups\n",
    "        self.shape_features = [\n",
    "            'min_y_shape', 'min_x_shape', 'max_y_shape', 'max_x_shape',\n",
    "            'min_f_shape', 'min_t_shape', 'max_f_shape', 'max_t_shape'\n",
    "        ]\n",
    "        self.shp_features = [f'shp_{i:03d}' for i in range(1, 17)]\n",
    "        self.centroid_features = [\n",
    "            'min_y_centroid', 'min_x_centroid', 'max_y_centroid', 'max_x_centroid',\n",
    "            'min_f_centroid', 'min_t_centroid', 'max_f_centroid', 'max_t_centroid',\n",
    "            'centroid_y', 'centroid_x'\n",
    "        ]\n",
    "        self.summary_features = [\n",
    "            'duration_x', 'bandwidth_y', 'area_xy', 'centroid_freq'\n",
    "        ]\n",
    "        self.delta_features = [\n",
    "            'delta_time', 'delta_freq', 'distance_between'\n",
    "        ]\n",
    "        \n",
    "        # Create main layout\n",
    "        self.create_main_layout()\n",
    "        \n",
    "        # Start monitoring the progress queue\n",
    "        self.master.after(100, self.check_progress_queue)\n",
    "    \n",
    "    def create_main_layout(self):\n",
    "        \"\"\"Create the main layout for the application.\"\"\"\n",
    "        # Main paned window - vertical split\n",
    "        main_paned = ttk.PanedWindow(self.master, orient=tk.VERTICAL)\n",
    "        main_paned.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Create top and bottom frames\n",
    "        top_frame = ttk.Frame(main_paned)\n",
    "        bottom_frame = ttk.Frame(main_paned)\n",
    "        \n",
    "        # Add frames to paned window\n",
    "        main_paned.add(top_frame, weight=1)\n",
    "        main_paned.add(bottom_frame, weight=2)\n",
    "        \n",
    "        # Top section - horizontal split\n",
    "        top_paned = ttk.PanedWindow(top_frame, orient=tk.HORIZONTAL)\n",
    "        top_paned.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Control and results frames\n",
    "        control_frame = ttk.LabelFrame(top_paned, text=\"Control Panel\")\n",
    "        results_frame = ttk.LabelFrame(top_paned, text=\"Cluster Results\")\n",
    "        \n",
    "        # Add frames to the horizontal pane\n",
    "        top_paned.add(control_frame, weight=2)\n",
    "        top_paned.add(results_frame, weight=1)\n",
    "        \n",
    "        # Set up control panel\n",
    "        self.create_control_panel(control_frame)\n",
    "        \n",
    "        # Set up results panel\n",
    "        self.create_results_panel(results_frame)\n",
    "        \n",
    "        # Set up visualization panel\n",
    "        self.create_viz_panel(bottom_frame)\n",
    "        \n",
    "        # Set up status bar\n",
    "        self.create_status_bar()\n",
    "    \n",
    "    def create_control_panel(self, parent):\n",
    "        \"\"\"Create the control panel with file selection, buttons, and settings tabs.\"\"\"\n",
    "        parent.columnconfigure(0, weight=1)\n",
    "        parent.columnconfigure(1, weight=1)\n",
    "        \n",
    "        # Left column - File selection and main controls\n",
    "        file_controls = ttk.Frame(parent)\n",
    "        file_controls.grid(row=0, column=0, sticky=\"nw\", padx=5, pady=5)\n",
    "        \n",
    "        # Right column - Settings tabs\n",
    "        settings_frame = ttk.Frame(parent)\n",
    "        settings_frame.grid(row=0, column=1, sticky=\"nsew\", padx=5, pady=5)\n",
    "        \n",
    "        # File input section\n",
    "        file_frame = ttk.LabelFrame(file_controls, text=\"Data Files\")\n",
    "        file_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Data file row\n",
    "        ttk.Label(file_frame, text=\"Data File:\").grid(row=0, column=0, sticky=\"w\", pady=2, padx=5)\n",
    "        self.file_path_var = tk.StringVar()\n",
    "        ttk.Entry(file_frame, textvariable=self.file_path_var, width=30).grid(row=0, column=1, sticky=\"ew\", pady=2)\n",
    "        ttk.Button(file_frame, text=\"Browse...\", command=self.browse_file).grid(row=0, column=2, sticky=\"e\", pady=2, padx=5)\n",
    "        \n",
    "        # Audio directory row\n",
    "        ttk.Label(file_frame, text=\"Audio Directory:\").grid(row=1, column=0, sticky=\"w\", pady=2, padx=5)\n",
    "        self.base_dir_var = tk.StringVar()\n",
    "        ttk.Entry(file_frame, textvariable=self.base_dir_var, width=30).grid(row=1, column=1, sticky=\"ew\", pady=2)\n",
    "        ttk.Button(file_frame, text=\"Browse...\", command=self.browse_dir).grid(row=1, column=2, sticky=\"e\", pady=2, padx=5)\n",
    "        \n",
    "        # Main control buttons\n",
    "        control_buttons = ttk.LabelFrame(file_controls, text=\"Main Controls\")\n",
    "        control_buttons.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Button(control_buttons, text=\"1. Load Data\", \n",
    "                  command=self.load_data, width=15).grid(row=0, column=0, padx=5, pady=10)\n",
    "        ttk.Button(control_buttons, text=\"2. Run Clustering\", \n",
    "                  command=self.run_clustering, width=15).grid(row=0, column=1, padx=5, pady=10)\n",
    "        ttk.Button(control_buttons, text=\"3. Update UMAP Plot\", \n",
    "                  command=self.update_umap_plot, width=15).grid(row=0, column=2, padx=5, pady=10)\n",
    "        \n",
    "        # Settings tabs\n",
    "        settings_notebook = ttk.Notebook(settings_frame)\n",
    "        settings_notebook.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Create settings tabs\n",
    "        self.create_umap_settings_tab(settings_notebook)\n",
    "        self.create_hdbscan_settings_tab(settings_notebook)\n",
    "        self.create_feature_settings_tab(settings_notebook)\n",
    "    \n",
    "    def create_umap_settings_tab(self, notebook):\n",
    "        \"\"\"Create the UMAP settings tab.\"\"\"\n",
    "        umap_tab = ttk.Frame(notebook, padding=10)\n",
    "        notebook.add(umap_tab, text=\"UMAP Settings\")\n",
    "        \n",
    "        # UMAP parameters\n",
    "        ttk.Label(umap_tab, text=\"n_neighbors:\").grid(row=0, column=0, sticky=\"w\", pady=2)\n",
    "        self.n_neighbors_var = tk.IntVar(value=15)\n",
    "        ttk.Spinbox(umap_tab, from_=2, to=100, textvariable=self.n_neighbors_var, width=5).grid(row=0, column=1, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(umap_tab, text=\"min_dist:\").grid(row=0, column=2, sticky=\"w\", pady=2)\n",
    "        self.min_dist_var = tk.DoubleVar(value=0.1)\n",
    "        ttk.Spinbox(umap_tab, from_=0.0, to=1.0, increment=0.05, textvariable=self.min_dist_var, width=5).grid(row=0, column=3, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(umap_tab, text=\"n_components:\").grid(row=1, column=0, sticky=\"w\", pady=2)\n",
    "        self.n_components_var = tk.IntVar(value=2)\n",
    "        ttk.Spinbox(umap_tab, from_=2, to=5, textvariable=self.n_components_var, width=5).grid(row=1, column=1, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(umap_tab, text=\"metric:\").grid(row=1, column=2, sticky=\"w\", pady=2)\n",
    "        self.metric_var = tk.StringVar(value=\"euclidean\")\n",
    "        metrics = [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\", \"cosine\", \"correlation\"]\n",
    "        ttk.Combobox(umap_tab, textvariable=self.metric_var, values=metrics, width=10).grid(row=1, column=3, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(umap_tab, text=\"spread:\").grid(row=2, column=0, sticky=\"w\", pady=2)\n",
    "        self.spread_var = tk.DoubleVar(value=1.0)\n",
    "        ttk.Spinbox(umap_tab, from_=0.1, to=5.0, increment=0.1, textvariable=self.spread_var, width=5).grid(row=2, column=1, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(umap_tab, text=\"local_connectivity:\").grid(row=2, column=2, sticky=\"w\", pady=2)\n",
    "        self.local_connectivity_var = tk.DoubleVar(value=1.0)\n",
    "        ttk.Spinbox(umap_tab, from_=0.5, to=2.0, increment=0.1, textvariable=self.local_connectivity_var, width=5).grid(row=2, column=3, pady=2, padx=5)\n",
    "    \n",
    "    def create_hdbscan_settings_tab(self, notebook):\n",
    "        \"\"\"Create the HDBSCAN settings tab.\"\"\"\n",
    "        hdbscan_tab = ttk.Frame(notebook, padding=10)\n",
    "        notebook.add(hdbscan_tab, text=\"HDBSCAN Settings\")\n",
    "        \n",
    "        ttk.Label(hdbscan_tab, text=\"min_cluster_size:\").grid(row=0, column=0, sticky=\"w\", pady=2)\n",
    "        self.min_cluster_size_var = tk.IntVar(value=15)\n",
    "        ttk.Spinbox(hdbscan_tab, from_=2, to=100, textvariable=self.min_cluster_size_var, width=5).grid(row=0, column=1, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(hdbscan_tab, text=\"min_samples:\").grid(row=0, column=2, sticky=\"w\", pady=2)\n",
    "        self.min_samples_var = tk.IntVar(value=5)\n",
    "        ttk.Spinbox(hdbscan_tab, from_=1, to=50, textvariable=self.min_samples_var, width=5).grid(row=0, column=3, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(hdbscan_tab, text=\"cluster_selection_epsilon:\").grid(row=1, column=0, sticky=\"w\", pady=2)\n",
    "        self.epsilon_var = tk.DoubleVar(value=0.0)\n",
    "        ttk.Spinbox(hdbscan_tab, from_=0.0, to=1.0, increment=0.05, textvariable=self.epsilon_var, width=5).grid(row=1, column=1, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(hdbscan_tab, text=\"alpha:\").grid(row=1, column=2, sticky=\"w\", pady=2)\n",
    "        self.alpha_var = tk.DoubleVar(value=1.0)\n",
    "        ttk.Spinbox(hdbscan_tab, from_=0.1, to=2.0, increment=0.1, textvariable=self.alpha_var, width=5).grid(row=1, column=3, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(hdbscan_tab, text=\"cluster_selection_method:\").grid(row=2, column=0, sticky=\"w\", pady=2)\n",
    "        self.selection_method_var = tk.StringVar(value=\"eom\")\n",
    "        methods = [\"eom\", \"leaf\"]\n",
    "        ttk.Combobox(hdbscan_tab, textvariable=self.selection_method_var, values=methods, width=10).grid(row=2, column=1, pady=2, padx=5)\n",
    "        \n",
    "        ttk.Label(hdbscan_tab, text=\"metric:\").grid(row=2, column=2, sticky=\"w\", pady=2)\n",
    "        self.hdbscan_metric_var = tk.StringVar(value=\"euclidean\")\n",
    "        h_metrics = [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\", \"cosine\", \"correlation\"]\n",
    "        ttk.Combobox(hdbscan_tab, textvariable=self.hdbscan_metric_var, values=h_metrics, width=10).grid(row=2, column=3, pady=2, padx=5)\n",
    "    \n",
    "    def create_feature_settings_tab(self, notebook):\n",
    "        \"\"\"Create the feature selection tab.\"\"\"\n",
    "        features_tab = ttk.Frame(notebook, padding=10)\n",
    "        notebook.add(features_tab, text=\"Feature Selection\")\n",
    "        \n",
    "        # Simple feature toggles\n",
    "        ttk.Label(features_tab, text=\"Include Feature Groups:\").grid(row=0, column=0, columnspan=2, sticky=\"w\", pady=5)\n",
    "        \n",
    "        # Group 1: Basic shape features\n",
    "        self.use_shape_var = tk.BooleanVar(value=True)\n",
    "        ttk.Checkbutton(features_tab, text=\"Basic Shape Features (min/max)\", variable=self.use_shape_var)\\\n",
    "            .grid(row=1, column=0, sticky=\"w\", padx=10)\n",
    "        \n",
    "        # Group 2: Shape descriptors\n",
    "        self.use_shp_var = tk.BooleanVar(value=True)\n",
    "        ttk.Checkbutton(features_tab, text=\"Shape Descriptors (shp_001-016)\", variable=self.use_shp_var)\\\n",
    "            .grid(row=1, column=1, sticky=\"w\", padx=10)\n",
    "        \n",
    "        # Group 3: Centroid features\n",
    "        self.use_centroid_var = tk.BooleanVar(value=True)\n",
    "        ttk.Checkbutton(features_tab, text=\"Centroid Features\", variable=self.use_centroid_var)\\\n",
    "            .grid(row=2, column=0, sticky=\"w\", padx=10)\n",
    "        \n",
    "        # Group 4: Summary metrics\n",
    "        self.use_summary_var = tk.BooleanVar(value=True)\n",
    "        ttk.Checkbutton(features_tab, text=\"Summary Metrics (duration, etc.)\", variable=self.use_summary_var)\\\n",
    "            .grid(row=2, column=1, sticky=\"w\", padx=10)\n",
    "        \n",
    "        # Group 5: Note pair features\n",
    "        self.use_pairs_var = tk.BooleanVar(value=False)\n",
    "        ttk.Checkbutton(features_tab, text=\"Note Pair Features\", variable=self.use_pairs_var)\\\n",
    "            .grid(row=3, column=0, sticky=\"w\", padx=10)\n",
    "        \n",
    "        # Group 6: Delta measurements\n",
    "        self.use_delta_var = tk.BooleanVar(value=False)\n",
    "        ttk.Checkbutton(features_tab, text=\"Delta Measurements\", variable=self.use_delta_var)\\\n",
    "            .grid(row=3, column=1, sticky=\"w\", padx=10)\n",
    "        \n",
    "        # Color selector\n",
    "        ttk.Label(features_tab, text=\"Color UMAP Plot By:\").grid(row=4, column=0, sticky=\"w\", pady=(15,2), padx=10)\n",
    "        self.color_by_var = tk.StringVar(value=\"cluster\")\n",
    "        self.color_by_combo = ttk.Combobox(features_tab, textvariable=self.color_by_var, width=20, state=\"readonly\")\n",
    "        self.color_by_combo.grid(row=4, column=1, sticky=\"w\", pady=(15,2), padx=10)\n",
    "        self.color_by_combo['values'] = ['cluster']  # Will be updated after data loads\n",
    "    \n",
    "    def create_results_panel(self, parent):\n",
    "        \"\"\"Create the results panel with cluster selection and statistics.\"\"\"\n",
    "        # Cluster selection\n",
    "        ttk.Label(parent, text=\"Select Cluster:\").grid(row=0, column=0, sticky=\"w\", pady=5)\n",
    "        self.cluster_var = tk.StringVar()\n",
    "        self.cluster_dropdown = ttk.Combobox(parent, textvariable=self.cluster_var, state=\"disabled\", width=10)\n",
    "        self.cluster_dropdown.grid(row=0, column=1, pady=5, padx=5)\n",
    "        self.cluster_dropdown.bind(\"<<ComboboxSelected>>\", self.on_cluster_selected)\n",
    "        \n",
    "        # Sample size\n",
    "        ttk.Label(parent, text=\"Samples to Display:\").grid(row=1, column=0, sticky=\"w\", pady=5)\n",
    "        self.sample_size_var = tk.IntVar(value=20)\n",
    "        ttk.Spinbox(parent, from_=1, to=100, textvariable=self.sample_size_var, width=5).grid(row=1, column=1, pady=5, padx=5)\n",
    "        ttk.Button(parent, text=\"Update Samples\", command=self.update_samples).grid(row=1, column=2, pady=5, padx=5)\n",
    "        \n",
    "        # Stats display\n",
    "        ttk.Label(parent, text=\"Cluster Statistics:\").grid(row=2, column=0, sticky=\"w\", pady=5)\n",
    "        self.stats_text = tk.Text(parent, width=40, height=15, wrap=tk.WORD)\n",
    "        self.stats_text.grid(row=3, column=0, columnspan=3, pady=5, sticky=\"nsew\")\n",
    "        self.stats_text.config(state=tk.DISABLED)\n",
    "        \n",
    "        # Make the text area expand\n",
    "        parent.grid_rowconfigure(3, weight=1)\n",
    "        parent.grid_columnconfigure(0, weight=1)\n",
    "    \n",
    "    def create_viz_panel(self, parent):\n",
    "        \"\"\"Create the visualization panel with UMAP and spectrograms tabs.\"\"\"\n",
    "        viz_frame = ttk.LabelFrame(parent, text=\"Visualizations\")\n",
    "        viz_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Create tabs for different visualizations\n",
    "        self.viz_tabs = ttk.Notebook(viz_frame)\n",
    "        self.viz_tabs.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # UMAP plot tab\n",
    "        self.umap_tab = ttk.Frame(self.viz_tabs)\n",
    "        self.viz_tabs.add(self.umap_tab, text=\"UMAP Plot\")\n",
    "        \n",
    "        # Controls for the plot\n",
    "        controls_frame = ttk.Frame(self.umap_tab)\n",
    "        controls_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        save_button = ttk.Button(controls_frame, text=\"Save Plot as PNG\", command=self.save_umap_plot, width=15)\n",
    "        save_button.pack(side=tk.RIGHT, padx=5, pady=2)\n",
    "        \n",
    "        # The plot itself\n",
    "        self.umap_fig = Figure(figsize=(10, 10), dpi=100)\n",
    "        self.umap_canvas = FigureCanvasTkAgg(self.umap_fig, self.umap_tab)\n",
    "        canvas_widget = self.umap_canvas.get_tk_widget()\n",
    "        canvas_widget.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Spectrograms tab\n",
    "        self.spectro_tab = ttk.Frame(self.viz_tabs)\n",
    "        self.viz_tabs.add(self.spectro_tab, text=\"Spectrograms\")\n",
    "        \n",
    "        # Create a canvas with scrollbar for spectrograms\n",
    "        self.spectro_canvas = tk.Canvas(self.spectro_tab)\n",
    "        scrollbar = ttk.Scrollbar(self.spectro_tab, orient=\"vertical\", command=self.spectro_canvas.yview)\n",
    "        self.spectro_frame = ttk.Frame(self.spectro_canvas)\n",
    "        \n",
    "        self.spectro_canvas.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "        self.spectro_canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        self.spectro_canvas.create_window((0, 0), window=self.spectro_frame, anchor=\"nw\")\n",
    "        \n",
    "        self.spectro_frame.bind(\"<Configure>\", lambda e: self.spectro_canvas.configure(scrollregion=self.spectro_canvas.bbox(\"all\")))\n",
    "    \n",
    "    def create_status_bar(self):\n",
    "        \"\"\"Create the status bar at the bottom of the window.\"\"\"\n",
    "        status_frame = ttk.Frame(self.master)\n",
    "        status_frame.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "        \n",
    "        self.status_var = tk.StringVar(value=\"Ready\")\n",
    "        status_label = ttk.Label(status_frame, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_label.pack(side=tk.LEFT, fill=tk.X, expand=True)\n",
    "        \n",
    "        self.progress_var = tk.IntVar(value=0)\n",
    "        self.progress_bar = ttk.Progressbar(status_frame, variable=self.progress_var,\n",
    "                                            mode='determinate', length=200)\n",
    "        self.progress_bar.pack(side=tk.RIGHT, padx=10)\n",
    "    \n",
    "    def browse_file(self):\n",
    "        \"\"\"Open file dialog to select CSV file.\"\"\"\n",
    "        filename = filedialog.askopenfilename(\n",
    "            title=\"Select CSV file with ROI data\",\n",
    "            filetypes=((\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\"))\n",
    "        )\n",
    "        if filename:\n",
    "            self.file_path_var.set(filename)\n",
    "    \n",
    "    def browse_dir(self):\n",
    "        \"\"\"Open directory dialog to select audio directory.\"\"\"\n",
    "        dirname = filedialog.askdirectory(title=\"Select base directory for audio files\")\n",
    "        if dirname:\n",
    "            self.base_dir_var.set(dirname)\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load data from CSV without running clustering.\"\"\"\n",
    "        file_path = self.file_path_var.get()\n",
    "        self.base_dir = self.base_dir_var.get()\n",
    "        \n",
    "        if not file_path or not os.path.exists(file_path):\n",
    "            messagebox.showerror(\"Error\", \"Please select a valid CSV file.\")\n",
    "            return\n",
    "        \n",
    "        if not self.base_dir or not os.path.exists(self.base_dir):\n",
    "            messagebox.showerror(\"Error\", \"Please select a valid audio directory.\")\n",
    "            return\n",
    "        \n",
    "        # Start loading in a separate thread\n",
    "        self.status_var.set(\"Loading data...\")\n",
    "        self.progress_var.set(0)\n",
    "        threading.Thread(target=self._load_data_thread, args=(file_path,), daemon=True).start()\n",
    "    \n",
    "    def _load_data_thread(self, file_path):\n",
    "        \"\"\"Thread function for data loading.\"\"\"\n",
    "        try:\n",
    "            self.progress_queue.put((\"status\", \"Loading data file...\"))\n",
    "            self.df = pd.read_csv(file_path)\n",
    "            \n",
    "            self.progress_queue.put((\"status\", \"Analyzing columns...\"))\n",
    "            self.progress_queue.put((\"progress\", 50))\n",
    "            \n",
    "            self._categorize_columns()\n",
    "            \n",
    "            self.progress_queue.put((\"update_columns\", None))\n",
    "            self.progress_queue.put((\"progress\", 100))\n",
    "            self.progress_queue.put((\"status\", f\"Data loaded: {len(self.df)} rows, {len(self.df.columns)} columns\"))\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.progress_queue.put((\"error\", f\"Error loading data: {str(e)}\"))\n",
    "    \n",
    "    def _categorize_columns(self):\n",
    "        \"\"\"Categorize columns as numeric or categorical for coloring options.\"\"\"\n",
    "        self.column_categories = {\n",
    "            'numeric': [],\n",
    "            'categorical': []\n",
    "        }\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            # Example logic: 'file_name' and 'site_name' -> categorical\n",
    "            if col in ['file_name', 'site_name']:\n",
    "                self.column_categories['categorical'].append(col)\n",
    "            elif self.df[col].dtype in [np.int64, np.float64]:\n",
    "                # If it has < 20 unique numeric values, treat as categorical\n",
    "                if len(self.df[col].unique()) < 20:\n",
    "                    self.column_categories['categorical'].append(col)\n",
    "                else:\n",
    "                    self.column_categories['numeric'].append(col)\n",
    "            else:\n",
    "                self.column_categories['categorical'].append(col)\n",
    "    \n",
    "    def update_columns_in_ui(self):\n",
    "        \"\"\"Update the color_by dropdown with available columns.\"\"\"\n",
    "        if self.df is not None:\n",
    "            all_options = []\n",
    "            \n",
    "            # Always include 'cluster' if we have cluster labels\n",
    "            if hasattr(self, 'clusters') and self.clusters is not None:\n",
    "                all_options.append('cluster')\n",
    "            \n",
    "            # Add everything from numeric/categorical\n",
    "            all_options += self.column_categories['categorical'] + self.column_categories['numeric']\n",
    "            \n",
    "            self.color_by_combo['values'] = all_options\n",
    "            \n",
    "            # Default selection\n",
    "            if 'cluster' in all_options:\n",
    "                self.color_by_var.set('cluster')\n",
    "            elif len(all_options) > 0:\n",
    "                self.color_by_var.set(all_options[0])\n",
    "    \n",
    "    def get_selected_features(self):\n",
    "        \"\"\"Get selected feature columns based on UI selections.\"\"\"\n",
    "        selected_features = []\n",
    "        \n",
    "        if self.use_shape_var.get():\n",
    "            selected_features.extend(self.shape_features)\n",
    "        \n",
    "        if self.use_shp_var.get():\n",
    "            selected_features.extend(self.shp_features)\n",
    "        \n",
    "        if self.use_centroid_var.get():\n",
    "            selected_features.extend(self.centroid_features)\n",
    "        \n",
    "        if self.use_summary_var.get():\n",
    "            selected_features.extend(self.summary_features)\n",
    "        \n",
    "        # Add note2 features if selected\n",
    "        if self.use_pairs_var.get():\n",
    "            base_features = []\n",
    "            if self.use_shp_var.get():\n",
    "                base_features.extend(self.shp_features)\n",
    "            if self.use_centroid_var.get():\n",
    "                # Exclude min_/max_ from centroid so we only get 'centroid_y/x' basically\n",
    "                base_features.extend(\n",
    "                    [f for f in self.centroid_features if not (f.startswith('min_') or f.startswith('max_'))]\n",
    "                )\n",
    "            if self.use_summary_var.get():\n",
    "                base_features.extend(self.summary_features)\n",
    "            \n",
    "            # Add note2_ prefix\n",
    "            note2_features = [f'note2_{col}' for col in base_features]\n",
    "            selected_features.extend(note2_features)\n",
    "        \n",
    "        # Add delta features if selected\n",
    "        if self.use_delta_var.get():\n",
    "            selected_features.extend(self.delta_features)\n",
    "        \n",
    "        if self.df is not None:\n",
    "            # Filter to only features present in DataFrame\n",
    "            available_features = [col for col in selected_features if col in self.df.columns]\n",
    "            missing = set(selected_features) - set(available_features)\n",
    "            if missing:\n",
    "                print(f\"Warning: Some selected features are not in the data: {missing}\")\n",
    "            return available_features\n",
    "        \n",
    "        return selected_features\n",
    "    \n",
    "    def run_clustering(self):\n",
    "        \"\"\"Run UMAP and HDBSCAN clustering.\"\"\"\n",
    "        file_path = self.file_path_var.get()\n",
    "        self.base_dir = self.base_dir_var.get()\n",
    "        \n",
    "        if not file_path or not os.path.exists(file_path):\n",
    "            messagebox.showerror(\"Error\", \"Please select a valid CSV file.\")\n",
    "            return\n",
    "        \n",
    "        if not self.base_dir or not os.path.exists(self.base_dir):\n",
    "            messagebox.showerror(\"Error\", \"Please select a valid audio directory.\")\n",
    "            return\n",
    "        \n",
    "        # Start in a separate thread\n",
    "        self.status_var.set(\"Loading data...\")\n",
    "        self.progress_var.set(0)\n",
    "        \n",
    "        threading.Thread(target=self.clustering_process, args=(file_path,), daemon=True).start()\n",
    "    \n",
    "    def clustering_process(self, file_path):\n",
    "        \"\"\"Process clustering in a separate thread.\"\"\"\n",
    "        try:\n",
    "            # Step 1: Load data if not already loaded\n",
    "            if self.df is None:\n",
    "                self.progress_queue.put((\"status\", \"Loading data...\"))\n",
    "                self.df = pd.read_csv(file_path)\n",
    "                self._categorize_columns()\n",
    "            \n",
    "            # Step 2: Get selected features\n",
    "            self.progress_queue.put((\"status\", \"Preparing features...\"))\n",
    "            self.progress_queue.put((\"progress\", 10))\n",
    "            feature_columns = self.get_selected_features()\n",
    "            \n",
    "            if not feature_columns:\n",
    "                self.progress_queue.put((\"error\", \"No features selected. Please select at least one feature group.\"))\n",
    "                return\n",
    "            \n",
    "            missing_columns = [col for col in feature_columns if col not in self.df.columns]\n",
    "            if missing_columns:\n",
    "                self.progress_queue.put((\"error\", f\"Missing columns in dataset: {missing_columns}\"))\n",
    "                return\n",
    "            \n",
    "            # Step 3: Prepare data\n",
    "            self.progress_queue.put((\"status\", \"Normalizing data...\"))\n",
    "            self.progress_queue.put((\"progress\", 20))\n",
    "            \n",
    "            X = self.df[feature_columns].copy()\n",
    "            X.fillna(0, inplace=True)  # fill NaN with 0\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            self.X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            # Step 4: UMAP\n",
    "            self.progress_queue.put((\"status\", \"Running UMAP...\"))\n",
    "            self.progress_queue.put((\"progress\", 30))\n",
    "            \n",
    "            umap_params = {\n",
    "                'n_neighbors': self.n_neighbors_var.get(),\n",
    "                'min_dist': self.min_dist_var.get(),\n",
    "                'n_components': self.n_components_var.get(),\n",
    "                'metric': self.metric_var.get(),\n",
    "                'spread': self.spread_var.get(),\n",
    "                'local_connectivity': self.local_connectivity_var.get(),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            self.reducer = umap.UMAP(**umap_params)\n",
    "            self.embedding = self.reducer.fit_transform(self.X_scaled)\n",
    "            \n",
    "            # Step 5: HDBSCAN\n",
    "            self.progress_queue.put((\"status\", \"Running HDBSCAN...\"))\n",
    "            self.progress_queue.put((\"progress\", 60))\n",
    "            \n",
    "            hdbscan_params = {\n",
    "                'min_cluster_size': self.min_cluster_size_var.get(),\n",
    "                'min_samples': self.min_samples_var.get(),\n",
    "                'cluster_selection_epsilon': self.epsilon_var.get(),\n",
    "                'alpha': self.alpha_var.get(),\n",
    "                'cluster_selection_method': self.selection_method_var.get(),\n",
    "                'metric': self.hdbscan_metric_var.get(),\n",
    "                'gen_min_span_tree': True\n",
    "            }\n",
    "            \n",
    "            self.clusterer = hdbscan.HDBSCAN(**hdbscan_params)\n",
    "            self.clusters = self.clusterer.fit_predict(self.embedding)\n",
    "            \n",
    "            # Step 6: Update DataFrame\n",
    "            self.progress_queue.put((\"status\", \"Processing results...\"))\n",
    "            self.progress_queue.put((\"progress\", 80))\n",
    "            \n",
    "            self.df['cluster'] = self.clusters\n",
    "            out_file = os.path.join(os.path.dirname(file_path), \"clustered_data_gui.csv\")\n",
    "            self.df.to_csv(out_file, index=False)\n",
    "            \n",
    "            # Step 7: UI updates\n",
    "            self.progress_queue.put((\"update_ui\", None))\n",
    "            self.progress_queue.put((\"progress\", 100))\n",
    "            self.progress_queue.put((\"status\", f\"Clustering complete. {len(np.unique(self.clusters))} clusters found.\"))\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.progress_queue.put((\"error\", f\"Error during clustering: {str(e)}\"))\n",
    "    \n",
    "    def check_progress_queue(self):\n",
    "        \"\"\"Check the queue for updates from the background thread.\"\"\"\n",
    "        try:\n",
    "            while True:\n",
    "                message, data = self.progress_queue.get_nowait()\n",
    "                \n",
    "                if message == \"status\":\n",
    "                    self.status_var.set(data)\n",
    "                elif message == \"progress\":\n",
    "                    self.progress_var.set(data)\n",
    "                elif message == \"error\":\n",
    "                    messagebox.showerror(\"Error\", data)\n",
    "                    self.status_var.set(\"Ready\")\n",
    "                elif message == \"update_ui\":\n",
    "                    self.update_ui_after_clustering()\n",
    "                elif message == \"update_columns\":\n",
    "                    self.update_columns_in_ui()\n",
    "                elif message == \"add_spectrogram\":\n",
    "                    self.add_spectrogram_to_ui(*data)\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "        \n",
    "        self.master.after(100, self.check_progress_queue)\n",
    "    \n",
    "    def update_ui_after_clustering(self):\n",
    "        \"\"\"Update UI after clustering completes.\"\"\"\n",
    "        unique_clusters = sorted(np.unique(self.clusters))\n",
    "        self.cluster_dropdown['values'] = unique_clusters\n",
    "        self.cluster_dropdown['state'] = 'readonly'\n",
    "        if unique_clusters:\n",
    "            self.cluster_dropdown.current(0)\n",
    "        \n",
    "        # Update color_by dropdown to include cluster\n",
    "        self.update_columns_in_ui()\n",
    "        \n",
    "        # Update UMAP plot\n",
    "        self.update_umap_plot()\n",
    "        \n",
    "        # Show stats\n",
    "        self.update_cluster_stats()\n",
    "    \n",
    "    def update_umap_plot(self):\n",
    "        \"\"\"Update the UMAP plot with clustering results, colored by the chosen column.\"\"\"\n",
    "        if self.embedding is None:\n",
    "            messagebox.showinfo(\"Information\", \"Please run clustering first or load precomputed data.\")\n",
    "            return\n",
    "        \n",
    "        self.umap_fig.clear()\n",
    "        ax = self.umap_fig.add_subplot(111)\n",
    "        ax.set_facecolor('#f8f8f8')\n",
    "        \n",
    "        color_by = self.color_by_var.get()\n",
    "        plot_title = 'UMAP Embedding'\n",
    "        \n",
    "        if color_by == 'cluster':\n",
    "            if self.clusters is not None:\n",
    "                color_data = self.clusters\n",
    "                cmap = 'viridis'\n",
    "                plot_title = 'UMAP Embedding with HDBSCAN Clustering'\n",
    "            else:\n",
    "                messagebox.showinfo(\"Information\", \"Clusters not available. Run clustering first.\")\n",
    "                return\n",
    "        elif color_by in self.df.columns:\n",
    "            color_data = self.df[color_by].values\n",
    "            # Numeric vs categorical\n",
    "            if color_by in self.column_categories['numeric']:\n",
    "                cmap = 'plasma'\n",
    "                plot_title = f'UMAP Embedding Colored by {color_by} (Numeric)'\n",
    "            else:\n",
    "                # For categorical, map to numeric\n",
    "                unique_vals = sorted(self.df[color_by].unique())\n",
    "                val_map = {val: i for i, val in enumerate(unique_vals)}\n",
    "                color_data = np.array([val_map[v] for v in color_data])\n",
    "                \n",
    "                cmap = 'tab20' if len(unique_vals) <= 20 else 'tab20b'\n",
    "                plot_title = f'UMAP Embedding Colored by {color_by} (Categorical)'\n",
    "                \n",
    "                # Custom legend\n",
    "                handles = []\n",
    "                for val, idx in val_map.items():\n",
    "                    color = plt.cm.get_cmap(cmap)(\n",
    "                        idx / (len(unique_vals) - 1 if len(unique_vals) > 1 else 1)\n",
    "                    )\n",
    "                    handles.append(plt.Line2D(\n",
    "                        [0], [0], marker='o', color='w',\n",
    "                        markerfacecolor=color, markersize=8, label=str(val)\n",
    "                    ))\n",
    "                ax.legend(handles=handles, title=color_by, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        else:\n",
    "            messagebox.showinfo(\"Information\", f\"Column {color_by} not found in data.\")\n",
    "            return\n",
    "        \n",
    "        # Scatter\n",
    "        scatter = ax.scatter(\n",
    "            self.embedding[:, 0], self.embedding[:, 1],\n",
    "            c=color_data, cmap=cmap, alpha=0.8, s=15, edgecolors='none'\n",
    "        )\n",
    "        \n",
    "        # For numeric data or cluster, add colorbar\n",
    "        if color_by == 'cluster' or color_by in self.column_categories['numeric']:\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.ax.tick_params(labelsize=9)\n",
    "        \n",
    "        ax.grid(True, linestyle='--', alpha=0.3, color='#cccccc')\n",
    "        ax.set_title(plot_title, fontsize=16, pad=15)\n",
    "        ax.set_xlabel('UMAP 1', fontsize=12)\n",
    "        ax.set_ylabel('UMAP 2', fontsize=12)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=9)\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('#aaaaaa')\n",
    "            spine.set_linewidth(0.5)\n",
    "        \n",
    "        self.umap_fig.tight_layout()\n",
    "        self.umap_canvas.draw()\n",
    "    \n",
    "    def save_umap_plot(self):\n",
    "        \"\"\"Save the current UMAP plot as a PNG file.\"\"\"\n",
    "        if self.embedding is None:\n",
    "            messagebox.showinfo(\"Information\", \"No UMAP plot available to save.\")\n",
    "            return\n",
    "        \n",
    "        save_path = filedialog.asksaveasfilename(\n",
    "            title=\"Save UMAP Plot\",\n",
    "            defaultextension=\".png\",\n",
    "            filetypes=((\"PNG files\", \"*.png\"), (\"All files\", \"*.*\"))\n",
    "        )\n",
    "        \n",
    "        if save_path:\n",
    "            try:\n",
    "                self.umap_fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                self.status_var.set(f\"Plot saved to {save_path}\")\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"Could not save plot: {str(e)}\")\n",
    "    \n",
    "    def update_cluster_stats(self):\n",
    "        \"\"\"Update the statistics text for the current cluster.\"\"\"\n",
    "        selected_cluster = self.cluster_var.get()\n",
    "        if not selected_cluster:\n",
    "            return\n",
    "        \n",
    "        selected_cluster = int(selected_cluster)\n",
    "        cluster_data = self.df[self.df['cluster'] == selected_cluster]\n",
    "        \n",
    "        stats = [\n",
    "            f\"Cluster: {selected_cluster}\",\n",
    "            f\"Number of ROIs: {len(cluster_data)}\",\n",
    "            f\"Percentage: {len(cluster_data) / len(self.df) * 100:.2f}%\"\n",
    "        ]\n",
    "        \n",
    "        if 'site_name' in self.df.columns:\n",
    "            site_counts = cluster_data['site_name'].value_counts()\n",
    "            stats.append(\"\\nSites:\")\n",
    "            for site, count in site_counts.items():\n",
    "                stats.append(f\"  {site}: {count} ({count / len(cluster_data)*100:.1f}%)\")\n",
    "        \n",
    "        stats.append(\"\\nAverage Feature Values:\")\n",
    "        key_features = ['duration_x', 'bandwidth_y', 'centroid_freq', 'area_xy']\n",
    "        for feature in key_features:\n",
    "            if feature in cluster_data.columns:\n",
    "                stats.append(f\"  {feature}: {cluster_data[feature].mean():.4f}\")\n",
    "        \n",
    "        self.stats_text.config(state=tk.NORMAL)\n",
    "        self.stats_text.delete(1.0, tk.END)\n",
    "        self.stats_text.insert(tk.END, \"\\n\".join(stats))\n",
    "        self.stats_text.config(state=tk.DISABLED)\n",
    "    \n",
    "    def on_cluster_selected(self, event):\n",
    "        \"\"\"Handle cluster dropdown selection.\"\"\"\n",
    "        self.update_cluster_stats()\n",
    "        self.update_samples()\n",
    "    \n",
    "    def update_samples(self):\n",
    "        \"\"\"Display spectrograms for the selected cluster.\"\"\"\n",
    "        selected_cluster = self.cluster_var.get()\n",
    "        if not selected_cluster:\n",
    "            return\n",
    "        \n",
    "        sample_size = self.sample_size_var.get()\n",
    "        selected_cluster = int(selected_cluster)\n",
    "        \n",
    "        # Clear spectrograms\n",
    "        for widget in self.spectro_frame.winfo_children():\n",
    "            widget.destroy()\n",
    "        \n",
    "        # Load in a separate thread\n",
    "        threading.Thread(\n",
    "            target=self.load_spectrograms,\n",
    "            args=(selected_cluster, sample_size),\n",
    "            daemon=True\n",
    "        ).start()\n",
    "    \n",
    "    def load_spectrograms(self, cluster, sample_size):\n",
    "        \"\"\"Load and display spectrograms for the selected cluster (threaded).\"\"\"\n",
    "        try:\n",
    "            self.progress_queue.put((\"status\", f\"Loading samples for cluster {cluster}...\"))\n",
    "            self.progress_queue.put((\"progress\", 0))\n",
    "            \n",
    "            cluster_data = self.df[self.df['cluster'] == cluster]\n",
    "            if len(cluster_data) > sample_size:\n",
    "                cluster_data = cluster_data.sample(sample_size, random_state=42)\n",
    "            \n",
    "            total_samples = len(cluster_data)\n",
    "            \n",
    "            for i, (_, row) in enumerate(cluster_data.iterrows()):\n",
    "                progress = int((i+1)/total_samples*100)\n",
    "                self.progress_queue.put((\"progress\", progress))\n",
    "                \n",
    "                file_name = row['file_name']\n",
    "                site_name = row.get('site_name', 'unknown')\n",
    "                \n",
    "                # Construct possible paths\n",
    "                possible_paths = [\n",
    "                    os.path.join(self.base_dir, site_name, 'snippets', file_name),\n",
    "                    os.path.join(self.base_dir, 'snippets', file_name),\n",
    "                    os.path.join(self.base_dir, file_name)\n",
    "                ]\n",
    "                \n",
    "                file_path = None\n",
    "                for path in possible_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        file_path = path\n",
    "                        break\n",
    "                \n",
    "                # Add to UI, either with or without spectrogram\n",
    "                self.progress_queue.put((\"add_spectrogram\", (file_path, row, i)))\n",
    "            \n",
    "            self.progress_queue.put((\"status\", f\"Loaded {total_samples} samples for cluster {cluster}\"))\n",
    "            self.progress_queue.put((\"progress\", 100))\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.progress_queue.put((\"error\", f\"Error loading spectrograms: {str(e)}\"))\n",
    "    \n",
    "    def add_spectrogram_to_ui(self, file_path, row, index):\n",
    "        \"\"\"\n",
    "        Add a single spectrogram widget to the Spectrograms tab.\n",
    "        - file_path: the resolved path to the audio file (or None if not found).\n",
    "        - row: the row of data (containing min_t_shape, etc.).\n",
    "        - index: index in the sample list, used for layout positioning.\n",
    "        \"\"\"\n",
    "        # Create a frame for this spectrogram\n",
    "        frame = ttk.Frame(self.spectro_frame, padding=5)\n",
    "        frame.grid(row=index // 2, column=index % 2, padx=5, pady=5, sticky=\"nsew\")\n",
    "        \n",
    "        file_name = row.get('file_name', 'Unknown File')\n",
    "        site_name = row.get('site_name', 'Unknown Site')\n",
    "        \n",
    "        ttk.Label(frame, text=f\"File: {file_name}\").grid(row=0, column=0, sticky=\"w\")\n",
    "        ttk.Label(frame, text=f\"Site: {site_name}\").grid(row=1, column=0, sticky=\"w\")\n",
    "        \n",
    "        # Create and embed the spectrogram\n",
    "        fig = self.create_spectrogram(file_path, row)\n",
    "        canvas = FigureCanvasTkAgg(fig, frame)\n",
    "        canvas.get_tk_widget().grid(row=2, column=0)\n",
    "        \n",
    "        # Add \"Play Audio\" or \"File Not Found\"\n",
    "        if file_path and os.path.exists(file_path):\n",
    "            play_btn = ttk.Button(frame, text=\"Play Audio\",\n",
    "                                  command=lambda fp=file_path: self.play_audio(fp))\n",
    "            play_btn.grid(row=3, column=0, pady=5)\n",
    "        else:\n",
    "            play_btn = ttk.Button(frame, text=\"File Not Found\", state=\"disabled\")\n",
    "            play_btn.grid(row=3, column=0, pady=5)\n",
    "    \n",
    "    def create_spectrogram(self, file_path, row):\n",
    "        \"\"\"Generate a spectrogram for the given file.\"\"\"\n",
    "        try:\n",
    "            if file_path is None:\n",
    "                # No file: show an error figure\n",
    "                fig = Figure(figsize=(4, 3), dpi=100)\n",
    "                ax = fig.add_subplot(111)\n",
    "                ax.text(0.5, 0.5, \"Audio file not found\",\n",
    "                        ha='center', va='center', fontsize=10)\n",
    "                ax.axis('off')\n",
    "                return fig\n",
    "            \n",
    "            # Load audio\n",
    "            s, fs = sound.load(file_path)\n",
    "            \n",
    "            # Compute spectrogram\n",
    "            Sxx, tn, fn, ext = sound.spectrogram(s, fs, nperseg=1024, noverlap=512)\n",
    "            Sxx_db = power2dB(Sxx) + 96\n",
    "            \n",
    "            fig = Figure(figsize=(4, 3), dpi=100)\n",
    "            ax = fig.add_subplot(111)\n",
    "            \n",
    "            # Plot spectrogram\n",
    "            plot2d(Sxx_db, ax=ax, extent=ext, vmin=0, vmax=70)\n",
    "            \n",
    "            # Add ROI rectangle if present\n",
    "            if all(c in row for c in ['min_t_shape','min_f_shape','max_t_shape','max_f_shape']):\n",
    "                rect = plt.Rectangle(\n",
    "                    (row['min_t_shape'], row['min_f_shape']),\n",
    "                    row['max_t_shape'] - row['min_t_shape'],\n",
    "                    row['max_f_shape'] - row['min_f_shape'],\n",
    "                    fill=False, edgecolor='yellow', linewidth=2\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "            \n",
    "            # Add second note rectangle if note2 columns exist\n",
    "            if self.use_pairs_var.get():\n",
    "                note2_keys = ['note2_min_t_shape','note2_min_f_shape','note2_max_t_shape','note2_max_f_shape']\n",
    "                if all(k in row for k in note2_keys) and pd.notna(row['note2_min_t_shape']):\n",
    "                    rect2 = plt.Rectangle(\n",
    "                        (row['note2_min_t_shape'], row['note2_min_f_shape']),\n",
    "                        row['note2_max_t_shape'] - row['note2_min_t_shape'],\n",
    "                        row['note2_max_f_shape'] - row['note2_min_f_shape'],\n",
    "                        fill=False, edgecolor='red', linewidth=2\n",
    "                    )\n",
    "                    ax.add_patch(rect2)\n",
    "            \n",
    "            cluster_label = row.get('cluster', 'Unknown')\n",
    "            ax.set_title(f\"Cluster {cluster_label}\", fontsize=10)\n",
    "            fig.tight_layout()\n",
    "            return fig\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Show an error figure\n",
    "            fig = Figure(figsize=(4, 3), dpi=100)\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.text(0.5, 0.5, f\"Error: {str(e)}\",\n",
    "                    ha='center', va='center', wrap=True)\n",
    "            ax.axis('off')\n",
    "            return fig\n",
    "    \n",
    "    def play_audio(self, file_path):\n",
    "        \"\"\"Play the audio file using sounddevice.\"\"\"\n",
    "        try:\n",
    "            data, samplerate = sf.read(file_path)\n",
    "            sd.play(data, samplerate)\n",
    "            self.status_var.set(f\"Playing audio: {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Cannot play audio: {str(e)}\")\n",
    "\n",
    "def start_gui():\n",
    "    root = tk.Tk()\n",
    "    app = ClusterAnalysisGUI(root)\n",
    "    \n",
    "    def on_closing():\n",
    "        root.destroy()\n",
    "    \n",
    "    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_gui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948095ee-06bd-4502-a249-ef5fc4441090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create plots without gui\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import sys\n",
    "\n",
    "# Set your desired parameters here\n",
    "UMAP_PARAMS = {\n",
    "    'n_neighbors': 20,\n",
    "    'min_dist': 0.1,\n",
    "    'n_components': 2\n",
    "}\n",
    "\n",
    "HDBSCAN_PARAMS = {\n",
    "    'min_cluster_size': 20,\n",
    "    'min_samples': 20  # Set to None if you want to use the default\n",
    "}\n",
    "\n",
    "# Function to create color map based on column values\n",
    "def create_color_map(values):\n",
    "    # Convert all values to strings to handle mixed types\n",
    "    str_values = values.astype(str)\n",
    "    unique_values = np.unique(str_values)\n",
    "    n_values = len(unique_values)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, n_values))\n",
    "    color_map = ListedColormap(colors)\n",
    "    value_to_int = {val: i for i, val in enumerate(unique_values)}\n",
    "    return color_map, value_to_int\n",
    "\n",
    "# Load the CSV file\n",
    "try:\n",
    "    df = pd.read_csv(r\"C:\\Users\\calla\\Dropbox\\2024\\powerfulowl\\paper_important_csv_and_figs\\clustered_data_custom_allroiverified.csv\")\n",
    "    print(f\"Successfully loaded CSV file. Shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the file path and ensure the file exists and is accessible.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Select relevant columns for clustering\n",
    "relevant_columns = [\n",
    "    # New columns\n",
    "    'min_y_shape', 'min_x_shape', 'max_y_shape', 'max_x_shape',\n",
    "    'min_f_shape', 'min_t_shape', 'max_f_shape', 'max_t_shape',\n",
    "    'shp_001', 'shp_002', 'shp_003', 'shp_004', 'shp_005', 'shp_006',\n",
    "    'shp_007', 'shp_008', 'shp_009', 'shp_010', 'shp_011', 'shp_012',\n",
    "    'shp_013', 'shp_014', 'shp_015', 'shp_016',\n",
    "    'min_y_centroid', 'min_x_centroid', 'max_y_centroid', 'max_x_centroid',\n",
    "    'min_f_centroid', 'min_t_centroid', 'max_f_centroid', 'max_t_centroid',\n",
    "    'centroid_y', 'centroid_x', 'duration_x', 'bandwidth_y', 'area_xy', 'centroid_freq',\n",
    "]\n",
    "\n",
    "# Check if all relevant columns are in the DataFrame\n",
    "missing_columns = [col for col in relevant_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: The following columns are missing from the DataFrame: {missing_columns}\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    sys.exit(1)\n",
    "\n",
    "# Prepare the data for clustering\n",
    "X = df[relevant_columns]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Apply UMAP\n",
    "print(\"Applying UMAP...\")\n",
    "reducer = umap.UMAP(**UMAP_PARAMS, random_state=42)\n",
    "embedding = reducer.fit_transform(X_normalized)\n",
    "\n",
    "# Apply HDBSCAN\n",
    "print(\"Applying HDBSCAN...\")\n",
    "clusterer = hdbscan.HDBSCAN(**HDBSCAN_PARAMS)\n",
    "clusters = clusterer.fit_predict(embedding)\n",
    "\n",
    "# Print cluster distribution information\n",
    "unique_clusters = np.unique(clusters)\n",
    "print(f\"\\nFound {len(unique_clusters)} unique cluster labels: {unique_clusters}\")\n",
    "print(f\"Number of noise points (cluster -1): {np.sum(clusters == -1)}\")\n",
    "cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "print(\"Cluster distribution:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "# Ask user for the column to use for coloring\n",
    "print(\"\\nAvailable columns for coloring:\")\n",
    "print(df.columns.tolist())\n",
    "color_column = input(\"Enter the name of the column to use for coloring: \")\n",
    "\n",
    "if color_column not in df.columns:\n",
    "    print(f\"Error: Column '{color_column}' not found in the DataFrame.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create color map based on the selected column\n",
    "color_map, value_to_int = create_color_map(df[color_column])\n",
    "color_values = [value_to_int[str(val)] for val in df[color_column]]\n",
    "\n",
    "# Visualize results\n",
    "print(\"Generating enhanced visualization...\")\n",
    "\n",
    "# Set the style for the plot\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"paper\", font_scale=1.4)  # Increased font scale for better readability\n",
    "\n",
    "# Create the figure and axis objects\n",
    "fig, ax = plt.subplots(figsize=(18, 12))  # Increased figure size for better visibility\n",
    "\n",
    "# Create the scatter plot\n",
    "scatter = ax.scatter(embedding[:, 0], embedding[:, 1], c=color_values, cmap=color_map,\n",
    "                     alpha=0.8, s=40, edgecolors='none')  # Slightly larger markers\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title(f'UMAP Embedding Colored by {color_column}', fontsize=20, fontweight='bold')\n",
    "ax.set_xlabel('UMAP 1', fontsize=16)\n",
    "ax.set_ylabel('UMAP 2', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)  # Larger tick labels\n",
    "\n",
    "# Add a colorbar legend with larger text\n",
    "cbar = plt.colorbar(scatter, ax=ax, aspect=40, pad=0.02)\n",
    "cbar.set_label(color_column, fontsize=16, fontweight='bold')  # Increased font size and made bold\n",
    "cbar.ax.tick_params(labelsize=14)  # Increased tick label size\n",
    "\n",
    "# If the column is categorical or has mixed types, set discrete ticks\n",
    "if df[color_column].dtype == 'object' or df[color_column].nunique() < 10 or df[color_column].dtype == 'float':\n",
    "    tick_locs = [value_to_int[str(val)] for val in value_to_int.keys()]\n",
    "    cbar.set_ticks(tick_locs)\n",
    "    cbar.set_ticklabels(list(value_to_int.keys()))\n",
    "\n",
    "# Add a text box with clustering parameters (larger font)\n",
    "params_text = f\"UMAP: n_neighbors={UMAP_PARAMS['n_neighbors']}, min_dist={UMAP_PARAMS['min_dist']}\\n\" \\\n",
    "              f\"HDBSCAN: min_cluster_size={HDBSCAN_PARAMS['min_cluster_size']}, \" \\\n",
    "              f\"min_samples={HDBSCAN_PARAMS['min_samples']}\"\n",
    "ax.text(0.05, 0.95, params_text, transform=ax.transAxes, fontsize=14,  # Increased font size\n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, \n",
    "                                           pad=0.6))  # Added more padding and increased opacity\n",
    "\n",
    "# Add grid lines for better readability\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure with higher resolution\n",
    "plt.savefig('cluster2_label.png', dpi=400, bbox_inches='tight')\n",
    "print(\"Figure saved as 'hdbscan_2nd_cluster.png'\")\n",
    "\n",
    "# Now create a second visualization showing the HDBSCAN clusters\n",
    "plt.figure(figsize=(18, 12))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, cmap='tab20', \n",
    "                      alpha=0.8, s=40, edgecolors='none')\n",
    "\n",
    "# Create a custom colormap that makes noise points grey\n",
    "cluster_cmap = plt.cm.get_cmap('tab20', len(np.unique(clusters)))\n",
    "cluster_colors = cluster_cmap(np.linspace(0, 1, len(np.unique(clusters))))\n",
    "if -1 in np.unique(clusters):\n",
    "    cluster_colors[0] = [0.7, 0.7, 0.7, 1.0]  # Grey for noise points (-1)\n",
    "custom_cmap = ListedColormap(cluster_colors)\n",
    "\n",
    "# Plot with custom colormap\n",
    "plt.figure(figsize=(18, 12))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "                      c=clusters, cmap=custom_cmap, \n",
    "                      alpha=0.8, s=40, edgecolors='none')\n",
    "\n",
    "plt.title('UMAP Embedding with HDBSCAN Clusters', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('UMAP 1', fontsize=16)\n",
    "plt.ylabel('UMAP 2', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "cbar = plt.colorbar(scatter, aspect=40, pad=0.02)\n",
    "cbar.set_label('Cluster', fontsize=16, fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "# Add a text box with clustering parameters\n",
    "plt.text(0.05, 0.95, params_text, transform=plt.gca().transAxes, fontsize=14,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, pad=0.6))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster2_hdbscan.png', dpi=400, bbox_inches='tight')\n",
    "print(\"Figure saved as 'hdbscan_clusters.png'\")\n",
    "\n",
    "# Show the plot (only if running interactively)\n",
    "plt.show()\n",
    "\n",
    "# Print top features for each cluster (skip noise cluster -1)\n",
    "print(\"\\nAnalyzing top features for each cluster...\")\n",
    "for cluster in tqdm(sorted([c for c in np.unique(clusters) if c != -1])):\n",
    "    cluster_data = X[clusters == cluster]\n",
    "    if len(cluster_data) > 0:  # Ensure we have data points in this cluster\n",
    "        cluster_mean = cluster_data.mean()\n",
    "        overall_mean = X.mean()\n",
    "        feature_importance = (cluster_mean - overall_mean) / overall_mean\n",
    "        # Replace NaN and infinite values with 0\n",
    "        feature_importance = feature_importance.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        top_features = feature_importance.nlargest(5)\n",
    "        print(f\"\\nCluster {cluster} (size: {len(cluster_data)}):\")\n",
    "        print(top_features)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df['HDBSCAN_cluster'] = clusters\n",
    "\n",
    "# Create a more user-friendly label for clusters in a new column\n",
    "# Convert to string with 'Noise' label for -1 values\n",
    "df['cluster_label'] = df['HDBSCAN_cluster'].apply(lambda x: 'Noise' if x == -1 else f'Cluster {x}')\n",
    "\n",
    "# Save results\n",
    "output_filename = 'clustered_data_custom.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nResults saved to '{output_filename}'\")\n",
    "\n",
    "# Print a summary of the clusters\n",
    "cluster_summary = df['cluster_label'].value_counts().sort_index()\n",
    "print(\"\\nCluster summary:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "print(\"\\nClustering completed. You can modify the UMAP_PARAMS and HDBSCAN_PARAMS at the top of the script to try different configurations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
