{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5172c42-3f8b-4e92-acc6-584de308e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Powerful Owl Call Detection using Deep Learning\n",
    "#This first notebook trains a binary call classifier, which we use to make predictions on long-duration audio before generating ROIS and clustering \n",
    "#there is a small example training dataset available to download in the supplementary materials we but encourage you to try on your own data\n",
    "#you can also use our models in any additional notebooks if you'd like to skip this step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b63464-367a-46b5-904c-c24d4c221ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate spectrograms\n",
    "#this code generates spectrograms from 5 second .wav file snippets to be used for training our binary classifier\n",
    "#place your audio files in a folder in 'positive' and 'negative' subfolders depending on whether it contains your code of interest\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import io\n",
    "\n",
    "# Parameters\n",
    "n_mels = 256   # Number of Mel bands\n",
    "n_fft = 4096  # FFT size\n",
    "hop_length = 512\n",
    "fmax = 8000\n",
    "img_size = (224, 224)  # Size for EfficientNet/ResNet\n",
    "brightness_factor = 0.8  # Factor to adjust brightness\n",
    "contrast_factor = 2.0  # Factor to adjust contrast\n",
    "\n",
    "# Choose your colormap here\n",
    "colormap = 'viridis'  # Options: 'viridis', 'inferno', 'magma', 'cividis', 'plasma'\n",
    "\n",
    "def create_spectrogram(filename, save_path, cmap):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(filename, sr=None)\n",
    "    \n",
    "    # Generate mel-spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, n_mels=n_mels, fmax=fmax, hop_length=hop_length)\n",
    "    S_DB = librosa.power_to_db(S, ref=np.max, amin=1e-10, top_db=80)\n",
    "    \n",
    "    # Create figure for the spectrogram\n",
    "    fig, ax = plt.subplots(figsize=(3.5, 3.5))\n",
    "    librosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', fmax=fmax, cmap=cmap, ax=ax, vmin=S_DB.max() - 80, vmax=S_DB.max())\n",
    "    ax.axis('off')  # Remove axes\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)  # Adjust margins to fill the figure\n",
    "\n",
    "    # Save the figure to a buffer\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', dpi=100, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Open the image and convert to RGB\n",
    "    img = Image.open(buf).convert('RGB')\n",
    "    img = img.resize(img_size)\n",
    "\n",
    "    # Adjust brightness and contrast\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    img = enhancer.enhance(brightness_factor)\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    img = enhancer.enhance(contrast_factor)\n",
    "\n",
    "    # Save the adjusted image\n",
    "    img.save(save_path)\n",
    "\n",
    "def process_directory(base_dir, sub_dirs, cmap):\n",
    "    for sub_dir in sub_dirs:\n",
    "        current_dir = os.path.join(base_dir, sub_dir)\n",
    "        save_dir = os.path.join(base_dir, 'spectrograms', sub_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        for filename in os.listdir(current_dir):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(current_dir, filename)\n",
    "                save_path = os.path.join(save_dir, os.path.splitext(filename)[0] + '.png')\n",
    "                create_spectrogram(file_path, save_path, cmap)\n",
    "                print(f\"Processed {filename}\")\n",
    "\n",
    "# Your directory setup\n",
    "#make sure your audio files are in negative and positive subfolders\n",
    "base_dir = r\"path_to_your_files\"\n",
    "sub_dirs = [\"positive\", \"negative\"]\n",
    "\n",
    "# Process directory with chosen colormap\n",
    "process_directory(base_dir, sub_dirs, colormap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d217be9-0828-43a5-8701-cf9620749a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hold out data for testing \n",
    "#this holds out a percentage of the training data for testing (10% here, you can change this by tweaking test_size\n",
    "#you can skip this step and use our test datasets available in the test folder\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def split_dataset(src_dir, test_size=0.10):\n",
    "    # Convert to Path object for easier handling\n",
    "    src_path = Path(src_dir)\n",
    "    \n",
    "    # Create test directory at same level as source directory\n",
    "    test_path = src_path.parent / 'test'\n",
    "    \n",
    "    # Create positive and negative directories in test folder\n",
    "    for class_name in ['positive', 'negative']:\n",
    "        test_class_path = test_path / class_name\n",
    "        test_class_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get list of all files in source class directory\n",
    "        src_class_path = src_path / class_name\n",
    "        all_files = list(src_class_path.glob('*'))\n",
    "        \n",
    "        # Calculate number of files to move to test\n",
    "        n_test = int(len(all_files) * test_size)\n",
    "        \n",
    "        # Randomly select files for test set\n",
    "        test_files = random.sample(all_files, n_test)\n",
    "        \n",
    "        # Move files to test directory\n",
    "        for file_path in test_files:\n",
    "            shutil.move(str(file_path), str(test_class_path / file_path.name))\n",
    "        \n",
    "        print(f\"Moved {n_test} files from {class_name} to test set\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Set source directory\n",
    "    src_dir = r\"path_to_your_spectrograms\"\n",
    "    \n",
    "    # Split the dataset\n",
    "    split_dataset(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ec4a5-93e1-497e-9546-3e42309c0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN MODEL\n",
    "#make sure your spectrograms are in folders named 'positive and 'negative'\n",
    "#you may need to experiment with tweaking the hyperparameters depending on your training data\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def create_mobilenet():\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    # Freeze all layers except the last few for fine-tuning\n",
    "    for layer in base_model.layers[:-30]:  \n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(256, activation='relu')(x)  # Reduced the size of the dense layer\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)  # Reduced dropout rate\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_mobilenet()\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setup Data Generators with appropriate data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,  \n",
    "    width_shift_range=0.1,  \n",
    "    height_shift_range=0.1,  \n",
    "    brightness_range=(0.8, 1.2),  \n",
    "    shear_range=0.05,  \n",
    "    zoom_range=0.1, \n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "base_dir = r'path_to_your_spectrograms'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "]\n",
    "\n",
    "# Start training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403fb108-a2c7-465c-88d9-a46d37a29eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model - you can then use your saved model for predictions and testing in the other notebooks\n",
    "model.save(r'path_to_your_model.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
